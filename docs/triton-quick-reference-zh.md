# Triton å¿«é€Ÿå‚è€ƒæ‰‹å†Œ

> **ä¸€é¡µçº¸äº†è§£ Triton æ ¸å¿ƒæ¦‚å¿µ**

## ä»€ä¹ˆæ˜¯ Tritonï¼Ÿ

**Triton** æ˜¯ä¸€ä¸ªç”¨ Python ç¼–å†™é«˜æ€§èƒ½ GPU å†…æ ¸çš„è¯­è¨€å’Œç¼–è¯‘å™¨ï¼Œç”± OpenAI å¼€å‘ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- ğŸš€ **é«˜ç”Ÿäº§åŠ›**ï¼šæ¯” CUDA ç®€å• 5-10 å€
- âš¡ **é«˜æ€§èƒ½**ï¼šè¾¾åˆ°æ‰‹å†™ CUDA çš„ 95-105%
- ğŸ”§ **é«˜çµæ´»æ€§**ï¼šæ”¯æŒè‡ªå®šä¹‰å’Œèåˆæ“ä½œ
- ğŸŒ **å¯ç§»æ¤**ï¼šåŒä¸€ä»£ç æ”¯æŒ NVIDIA å’Œ AMD GPU

---

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install triton
```

### Hello Worldï¼ˆå‘é‡åŠ æ³•ï¼‰

```python
import torch
import triton
import triton.language as tl

@triton.jit
def add_kernel(x_ptr, y_ptr, out_ptr, n, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offs < n
    x = tl.load(x_ptr + offs, mask=mask)
    y = tl.load(y_ptr + offs, mask=mask)
    tl.store(out_ptr + offs, x + y, mask=mask)

def add(x, y):
    out = torch.empty_like(x)
    n = out.numel()
    grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)
    add_kernel[grid](x, y, out, n, BLOCK_SIZE=1024)
    return out

# ä½¿ç”¨
x = torch.rand(10000, device='cuda')
y = torch.rand(10000, device='cuda')
z = add(x, y)
```

---

## æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥

### SPMD ç¼–ç¨‹æ¨¡å‹

| CUDA | Triton |
|------|--------|
| æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªå…ƒç´  | æ¯ä¸ªç¨‹åºå¤„ç†ä¸€ä¸ªæ•°æ®å— |
| æ‰‹åŠ¨ç®¡ç†å…±äº«å†…å­˜ | ç¼–è¯‘å™¨è‡ªåŠ¨ç®¡ç† |
| æ˜¾å¼åŒæ­¥ | è‡ªåŠ¨åŒæ­¥ |

### å…³é”®è¯­è¨€åŸè¯­

```python
# ç¨‹åºæ§åˆ¶
pid = tl.program_id(axis=0)        # è·å–ç¨‹åº ID
offsets = tl.arange(0, BLOCK_SIZE) # ç”Ÿæˆç´¢å¼•èŒƒå›´

# å†…å­˜æ“ä½œ
data = tl.load(ptr, mask=mask)     # åŠ è½½æ•°æ®
tl.store(ptr, data, mask=mask)     # å­˜å‚¨æ•°æ®
tl.atomic_add(ptr, data)           # åŸå­æ“ä½œ

# è®¡ç®—æ“ä½œ
result = tl.dot(a, b)              # çŸ©é˜µä¹˜æ³•
result = tl.sum(x, axis=0)         # å½’çº¦æ±‚å’Œ
result = tl.exp(x)                 # å…ƒç´ çº§å‡½æ•°
```

### è‡ªåŠ¨è°ƒä¼˜

```python
@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),
        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),
    ],
    key=['n'],
)
@triton.jit
def kernel(...):
    ...
```

---

## ä¸ CUDA ç”Ÿæ€å¯¹æ¯”

### cuBLAS

| | cuBLAS | Triton |
|---|--------|--------|
| **ç”¨é€”** | æ ‡å‡† BLAS æ“ä½œ | è‡ªå®šä¹‰æ“ä½œ |
| **çµæ´»æ€§** | ä½ï¼ˆå›ºå®šç®—æ³•ï¼‰ | é«˜ï¼ˆå®Œå…¨å¯å®šåˆ¶ï¼‰ |
| **æ€§èƒ½** | å³°å€¼ï¼ˆ100%ï¼‰ | æ¥è¿‘å³°å€¼ï¼ˆ95-105%ï¼‰ |
| **ä½¿ç”¨å»ºè®®** | æ ‡å‡†çŸ©é˜µä¹˜æ³• | èåˆæ“ä½œ |

### CUTLASS

| | CUTLASS | Triton |
|---|---------|--------|
| **è¯­è¨€** | C++ æ¨¡æ¿ | Python |
| **å­¦ä¹ æ›²çº¿** | é™¡å³­ | å¹³ç¼“ |
| **å¼€å‘é€Ÿåº¦** | æ…¢ | å¿« |
| **æ€§èƒ½** | å³°å€¼ | æ¥è¿‘å³°å€¼ |
| **ä½¿ç”¨å»ºè®®** | æè‡´ä¼˜åŒ– | å¿«é€Ÿå¼€å‘ |

### cuDNN

| | cuDNN | Triton |
|---|-------|--------|
| **è¦†ç›–èŒƒå›´** | DNN æ ‡å‡†å±‚ | é€šç”¨å†…æ ¸ |
| **å®šåˆ¶æ€§** | ä½ï¼ˆé»‘ç›’ï¼‰ | é«˜ï¼ˆç™½ç›’ï¼‰ |
| **ä½¿ç”¨å»ºè®®** | Convã€BN ç­‰æ ‡å‡†å±‚ | è‡ªå®šä¹‰å±‚å’Œèåˆæ“ä½œ |

---

## å…¸å‹åº”ç”¨åœºæ™¯

### âœ… é€‚åˆä½¿ç”¨ Triton

1. **èåˆç®—å­**ï¼šå‡å°‘å†…å­˜è®¿é—®
   ```python
   # èåˆ GELU(matmul(x, w) + b)
   @triton.jit
   def fused_matmul_gelu(...)
   ```

2. **è‡ªå®šä¹‰æ“ä½œ**ï¼šå®ç°æ–°ç®—æ³•
   ```python
   # Flash Attention
   # Grouped GEMM
   # Custom Normalization
   ```

3. **ç‰¹æ®Šä¼˜åŒ–**ï¼šé’ˆå¯¹ç‰¹å®šæ•°æ®åˆ†å¸ƒ
   ```python
   # ç¨€ç–çŸ©é˜µä¹˜æ³•
   # å—ç¨€ç–æ³¨æ„åŠ›
   ```

### âŒ ä¸é€‚åˆä½¿ç”¨ Triton

1. **æ ‡å‡†æ“ä½œ**ï¼šå·²æœ‰ä¼˜åŒ–åº“
   ```python
   # ä½¿ç”¨ torch.matmulï¼ˆåº•å±‚ cuBLASï¼‰
   C = torch.matmul(A, B)
   ```

2. **å¤æ‚æ§åˆ¶æµ**ï¼šå¯èƒ½ç¼–è¯‘å¤±è´¥

3. **æœ€å 1% ä¼˜åŒ–**ï¼šè€ƒè™‘ CUTLASS

---

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. é€‰æ‹©åˆé€‚çš„å—å¤§å°

```python
# ç»éªŒæ³•åˆ™
BLOCK_SIZE = 128 æˆ– 256  # ä¸€èˆ¬æƒ…å†µ
BLOCK_SIZE = 64 æˆ– 32    # å†…å­˜å¯†é›†å‹
```

### 2. ä½¿ç”¨è‡ªåŠ¨è°ƒä¼˜

```python
@triton.autotune(configs=[...], key=['M', 'N', 'K'])
```

### 3. å†…å­˜è®¿é—®ä¼˜åŒ–

```python
# âœ… åˆå¹¶è®¿é—®
offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)

# âŒ è·¨æ­¥è®¿é—®ï¼ˆæ…¢ï¼‰
offs = pid + tl.arange(0, N) * stride
```

### 4. åˆ©ç”¨ Tensor Core

```python
# ä½¿ç”¨ tl.dot() è¿›è¡ŒçŸ©é˜µä¹˜æ³•
acc = tl.dot(a, b, acc)  # è‡ªåŠ¨ä½¿ç”¨ Tensor Core
```

---

## è°ƒè¯•æŠ€å·§

### ç¯å¢ƒå˜é‡

```bash
# æ‰“å°ç”Ÿæˆçš„ IR
export MLIR_ENABLE_DUMP=1

# ä½¿ç”¨è§£é‡Šå™¨ï¼ˆä¸éœ€è¦ GPUï¼‰
export TRITON_INTERPRET=1

# ä¿å­˜ç¼–è¯‘ç»“æœ
export TRITON_KERNEL_DUMP=1
export TRITON_DUMP_DIR=./dump
```

### Python è°ƒè¯•

```python
# æŸ¥çœ‹ç”Ÿæˆçš„ PTX
print(kernel.asm['ptx'])

# æ€§èƒ½æµ‹è¯•
triton.testing.do_bench(lambda: kernel[grid](...))
```

---

## ä»£ç æ¡†æ¶é€Ÿè§ˆ

```
triton/
â”œâ”€â”€ python/triton/           # Python å‰ç«¯
â”‚   â”œâ”€â”€ language/           # è¯­è¨€å®šä¹‰ (tl.*)
â”‚   â”œâ”€â”€ compiler/           # ç¼–è¯‘å™¨å‰ç«¯
â”‚   â”œâ”€â”€ runtime/            # JIT å’Œè‡ªåŠ¨è°ƒä¼˜
â”‚   â””â”€â”€ backends/           # ç¡¬ä»¶åç«¯
â”œâ”€â”€ lib/                    # C++ åç«¯ï¼ˆMLIRï¼‰
â”‚   â”œâ”€â”€ Dialect/           # MLIR æ–¹è¨€
â”‚   â””â”€â”€ Conversion/        # æ–¹è¨€è½¬æ¢
â””â”€â”€ docs/                   # æ–‡æ¡£
```

---

## ç¼–è¯‘æµç¨‹

```
Python ä»£ç 
   â†“
Triton IR (é«˜å±‚)
   â†“
TritonGPU IR (GPU ä¼˜åŒ–)
   â†“
LLVM IR
   â†“
PTX/GCN (æ±‡ç¼–)
   â†“
äºŒè¿›åˆ¶ä»£ç 
```

---

## å­¦ä¹ èµ„æº

### å®˜æ–¹èµ„æº
- **å®˜ç½‘**ï¼šhttps://triton-lang.org
- **GitHub**ï¼šhttps://github.com/triton-lang/triton
- **æ•™ç¨‹**ï¼š`python/tutorials/` ç›®å½•

### æ¨èå­¦ä¹ è·¯å¾„

1. **å…¥é—¨**ï¼ˆ1-2 å¤©ï¼‰
   - é˜…è¯» `01-vector-add.py`
   - è¿è¡Œ `02-fused-softmax.py`
   - ç†è§£ SPMD æ¨¡å‹

2. **è¿›é˜¶**ï¼ˆ1-2 å‘¨ï¼‰
   - å®ç° `03-matrix-multiplication.py`
   - å­¦ä¹ è‡ªåŠ¨è°ƒä¼˜
   - ä¼˜åŒ–å†…å­˜è®¿é—®

3. **é«˜çº§**ï¼ˆæŒç»­ï¼‰
   - é˜…è¯»ç¼–è¯‘å™¨æºç 
   - è´¡çŒ®æ–°ç‰¹æ€§
   - ç ”ç©¶ MLIR æ–¹è¨€

### ç¤¾åŒºèµ„æº
- **Triton Puzzles**ï¼šhttps://github.com/srush/Triton-Puzzles
- **ä¼šè®®è§†é¢‘**ï¼šYouTube "Triton Developer Conference"

---

## å¸¸è§é—®é¢˜ FAQ

### Q: Triton ä¼šæ›¿ä»£ CUDA å—ï¼Ÿ
**A**: ä¸ä¼šã€‚Triton æ˜¯ CUDA çš„**è¡¥å……**ï¼Œç”¨äºç®€åŒ–è‡ªå®šä¹‰å†…æ ¸å¼€å‘ã€‚

### Q: Triton çš„æ€§èƒ½å¦‚ä½•ï¼Ÿ
**A**: é€šå¸¸è¾¾åˆ°æ‰‹å†™ CUDA çš„ 95-105%ï¼Œå¯¹äºæŸäº›æ“ä½œç”šè‡³æ›´å¥½ã€‚

### Q: å­¦ä¹  Triton éœ€è¦äº†è§£ CUDA å—ï¼Ÿ
**A**: ä¸éœ€è¦ã€‚äº†è§£åŸºæœ¬çš„å¹¶è¡Œè®¡ç®—æ¦‚å¿µå³å¯ï¼Œä½†äº†è§£ CUDA æœ‰åŠ©äºç†è§£åº•å±‚ã€‚

### Q: Triton æ”¯æŒ CPU å—ï¼Ÿ
**A**: æ­£åœ¨å¼€å‘ä¸­ã€‚ç›®å‰ä¸»è¦æ”¯æŒ NVIDIA å’Œ AMD GPUã€‚

### Q: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ Tritonï¼Ÿ
**A**: è®¸å¤šå…¬å¸ï¼ˆåŒ…æ‹¬ OpenAIï¼‰å·²åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚å»ºè®®å…ˆåœ¨éå…³é”®è·¯å¾„æµ‹è¯•ã€‚

---

## å®ç”¨ä»£ç ç‰‡æ®µ

### åŸºæœ¬æ¨¡æ¿

```python
import triton
import triton.language as tl

@triton.jit
def my_kernel(
    # æŒ‡é’ˆå‚æ•°
    input_ptr, output_ptr,
    # å½¢çŠ¶å‚æ•°
    n_elements,
    # å…ƒå‚æ•°ï¼ˆç¼–è¯‘æ—¶å¸¸é‡ï¼‰
    BLOCK_SIZE: tl.constexpr,
):
    # 1. è·å–ç¨‹åº ID
    pid = tl.program_id(0)
    
    # 2. è®¡ç®—åç§»é‡
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    
    # 3. åˆ›å»ºæ©ç 
    mask = offsets < n_elements
    
    # 4. åŠ è½½æ•°æ®
    data = tl.load(input_ptr + offsets, mask=mask)
    
    # 5. è®¡ç®—
    result = data * 2.0
    
    # 6. å­˜å‚¨ç»“æœ
    tl.store(output_ptr + offsets, result, mask=mask)
```

### å¯åŠ¨å†…æ ¸

```python
def launch_kernel(input_tensor):
    output = torch.empty_like(input_tensor)
    n = output.numel()
    
    grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)
    
    my_kernel[grid](
        input_tensor, output, n,
        BLOCK_SIZE=1024
    )
    
    return output
```

---

## æ€§èƒ½å¯¹æ¯”æ€»ç»“

| æ“ä½œ | cuBLAS/cuDNN | æ‰‹å†™ CUDA | Triton | å¼€å‘æ—¶é—´ |
|------|--------------|-----------|--------|---------|
| çŸ©é˜µä¹˜æ³• | 100% | 95-100% | 95-105% | 1-2 å°æ—¶ |
| Softmax | - | 100% | 100-120% | 30 åˆ†é’Ÿ |
| LayerNorm | 100% | 100% | 90-110% | 1 å°æ—¶ |
| èåˆç®—å­ | - | 100% | 95-105% | 2-4 å°æ—¶ |

**ç»“è®º**ï¼šTriton æä¾›äº†**ç”Ÿäº§åŠ›**å’Œ**æ€§èƒ½**çš„æœ€ä½³å¹³è¡¡ã€‚

---

**ç‰ˆæœ¬**ï¼šTriton 3.6.0  
**æ›´æ–°**ï¼š2024å¹´12æœˆ

æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…å®Œæ•´æŒ‡å—ï¼š`triton-framework-guide-zh.md`
